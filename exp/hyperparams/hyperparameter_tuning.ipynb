{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook explores the effect of various hyperparameters on the performance of SCINet when training on cryptocurrency data, particularly Bitcoin. We will here only address those hyperparameters that are directly accessible via the 'train_scinet' or 'preprocess' functions. Of course, we are aware that effects of different hyperparameters may correlate which means the per parameter sweep as performed here is suboptimal. That said, this approach does grant a lot of insight into the way the different hyperparameters influence the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First the necessary modules en scripts are imported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "\n",
    "cwd = os.getcwd()\n",
    "BASE_DIR = os.path.dirname(os.path.dirname(cwd))\n",
    "\n",
    "sys.path.insert(0, BASE_DIR) #add base to path for relative imports\n",
    "os.chdir('../..')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Per sample versus per column normalisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we will investigate is the effect of the method of normalisation on the data. That is, we can either normalize every sample individually or the column as a whole. In the latter case, we apply a log function before normalising to deal with the different orders of magnitude in some features (this is the case although it is not explicitly shown here). Also, an option to skip the training (and use precomputed models instead) is available in the form of 'train'. Set it to to 'true' in case you want to train explicitly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting data preprocessing...\n",
      "   48740.22  48745.96  48727.47  48727.47.1   2.27206  110730.9135  136.0\n",
      "0  48763.11  48763.12  48736.70    48736.73   5.33108  259880.1205  427.0\n",
      "1  48778.58  48778.58  48750.37    48763.12   6.87389  335219.0368  389.0\n",
      "2  48760.37  48778.58  48746.39    48778.58  10.58951  516291.2896  425.0\n",
      "3  48799.99  48800.00  48756.93    48760.37  12.24525  597357.8390  535.0\n",
      "4  48795.99  48800.00  48795.99    48800.00   7.55759  368810.1891  423.0 (49997, 7)\n",
      "Making train/validation/test splits...\n",
      "Making samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34717/34717 [00:25<00:00, 1387.09it/s]\n",
      "c:\\Users\\PimVeefkind\\Documents\\ADL-Scinet\\SCINet\\exp\\hyperparams\\utils\\preprocess_data.py:143: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  samples = np.array(samples)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7220/7220 [00:05<00:00, 1219.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7220/7220 [00:03<00:00, 1865.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making X-y splits...\n",
      "Starting data preprocessing...\n",
      "   48740.22  48745.96  48727.47  48727.47.1   2.27206  110730.9135  136.0\n",
      "0  48763.11  48763.12  48736.70    48736.73   5.33108  259880.1205  427.0\n",
      "1  48778.58  48778.58  48750.37    48763.12   6.87389  335219.0368  389.0\n",
      "2  48760.37  48778.58  48746.39    48778.58  10.58951  516291.2896  425.0\n",
      "3  48799.99  48800.00  48756.93    48760.37  12.24525  597357.8390  535.0\n",
      "4  48795.99  48800.00  48795.99    48800.00   7.55759  368810.1891  423.0 (49997, 7)\n",
      "Making train/validation/test splits...\n",
      "Making samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34717/34717 [00:20<00:00, 1673.17it/s]\n",
      "c:\\Users\\PimVeefkind\\Documents\\ADL-Scinet\\SCINet\\exp\\hyperparams\\utils\\preprocess_data.py:143: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  samples = np.array(samples)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7220/7220 [00:03<00:00, 2033.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7220/7220 [00:03<00:00, 1988.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making X-y splits...\n"
     ]
    }
   ],
   "source": [
    "from base.train_scinet import train_scinet\n",
    "from utils.data_loading import load_data\n",
    "from utils.preprocess_data import preprocess\n",
    "\n",
    "data_format=[\"open\",\"high\",\"low\",\"close\",\"Volume BTC\",\"Volume USDT\",\"tradecount\"]\n",
    "\n",
    "X_len = 256\n",
    "Y_len = 24\n",
    "\n",
    "sampling = [True,False]\n",
    "\n",
    "means = []\n",
    "stds = []\n",
    "\n",
    "data_test = {}\n",
    "\n",
    "for normalize_per_sample in sampling:\n",
    "\n",
    "    standardization_settings = {'per_sample': normalize_per_sample,\n",
    "                            'leaky': False,\n",
    "                            'mode': 'log', #only if per sample is false, choose from log, sqrt or lin\n",
    "                            'sqrt_val': 2, #of course only if mode is sqrt\n",
    "                            'total mean': [],\n",
    "                            'total std': []}\n",
    "\n",
    "    pairs = [\"HUFL\", \"HULL\", \"MUFL\", \"MULL\", \"LUFL\", \"LULL\", \"OT\"]\n",
    " \n",
    "    data, mean ,std = load_data('Binance_BTCUSDT_minute', pairs)\n",
    "\n",
    "    \n",
    "\n",
    "    data_proc = preprocess(   data = data,\n",
    "                        symbols = pairs,\n",
    "                        data_format = data_format,\n",
    "                        fraction = 1,\n",
    "                        train_frac = .7,\n",
    "                        val_frac = .15,\n",
    "                        test_frac = .15,\n",
    "                        X_LEN = X_len,\n",
    "                        Y_LEN = Y_len,\n",
    "                        OVERLAPPING = True,\n",
    "                        STANDARDIZE = True,\n",
    "                        standardization_settings = standardization_settings\n",
    "                        )\n",
    "\n",
    "    means.append(data_proc['mean'])\n",
    "    stds.append(data_proc['std'])\n",
    "\n",
    "    data_test[str(normalize_per_sample)]= {'X_test': data_proc['X_test'],'y_test_unnormalized': data_proc['y_test_unnormalized']}\n",
    "\n",
    "    train = False\n",
    "    if train:\n",
    "        results = train_scinet( X_train = data_proc[\"X_train\"].astype('float32'),\n",
    "                            y_train = data_proc[\"y_train\"].astype('float32'),\n",
    "                            X_val = data_proc[\"X_val\"].astype('float32'),\n",
    "                            y_val = data_proc[\"y_val\"].astype('float32'),\n",
    "                            X_test = data_proc[\"X_test\"].astype('float32'),\n",
    "                            y_test = data_proc[\"y_test\"].astype('float32'),\n",
    "                            epochs = 1,\n",
    "                            batch_size = 128,\n",
    "                            X_LEN = X_len,\n",
    "                            Y_LEN = [Y_len],\n",
    "                            output_dim = [data_proc[\"X_train\"].shape[2]],\n",
    "                            selected_columns = [[]],\n",
    "                            hid_size= 8,\n",
    "                            num_levels= 4,\n",
    "                            kernel = 5,\n",
    "                            dropout = .5,\n",
    "                            loss_weights= [1],\n",
    "                            learning_rate = 0.001,\n",
    "                            probabilistic = False)\n",
    "\n",
    "        results[0].save_weigths(f'/exp/hyperparams/saved_models/model_sample_{str(normalize_per_sample)}.h5')\n",
    "\n",
    "        \n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to properly compare the performances, we should denormalize the predictions. Otherwise, the best performer will be the one where the normalisations yielded the smallest values. Here, we will only predict the first column as it shortens training considerably. In addition, since we have to denormalize we cannot fairly calculate a mae over all columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "No file or directory found at exp/hyperparams/saved_models/model_sample_True",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\PimVeefkind\\Documents\\ADL-Scinet\\SCINet\\exp\\hyperparams\\hyperparameter_tuning.ipynb Cell 9'\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/PimVeefkind/Documents/ADL-Scinet/SCINet/exp/hyperparams/hyperparameter_tuning.ipynb#ch0000024?line=13'>14</a>\u001b[0m maes \u001b[39m=\u001b[39m []\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/PimVeefkind/Documents/ADL-Scinet/SCINet/exp/hyperparams/hyperparameter_tuning.ipynb#ch0000024?line=15'>16</a>\u001b[0m \u001b[39mfor\u001b[39;00m normalize_per_sample \u001b[39min\u001b[39;00m sampling:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/PimVeefkind/Documents/ADL-Scinet/SCINet/exp/hyperparams/hyperparameter_tuning.ipynb#ch0000024?line=17'>18</a>\u001b[0m     model \u001b[39m=\u001b[39m keras\u001b[39m.\u001b[39;49mmodels\u001b[39m.\u001b[39;49mload_model(\u001b[39m'\u001b[39;49m\u001b[39mexp/hyperparams/saved_models/model_sample_\u001b[39;49m\u001b[39m{}\u001b[39;49;00m\u001b[39m'\u001b[39;49m\u001b[39m.\u001b[39;49mformat(\u001b[39mstr\u001b[39;49m(normalize_per_sample)))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/PimVeefkind/Documents/ADL-Scinet/SCINet/exp/hyperparams/hyperparameter_tuning.ipynb#ch0000024?line=18'>19</a>\u001b[0m     prediction \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(data_proc[\u001b[39m'\u001b[39m\u001b[39mX_test\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/PimVeefkind/Documents/ADL-Scinet/SCINet/exp/hyperparams/hyperparameter_tuning.ipynb#ch0000024?line=19'>20</a>\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(data_test[\u001b[39mstr\u001b[39m(normalize_per_sample)][\u001b[39m'\u001b[39m\u001b[39mX_test\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mshape[\u001b[39m2\u001b[39m])):\n",
      "File \u001b[1;32mc:\\Users\\PimVeefkind\\miniconda3\\envs\\learn\\lib\\site-packages\\keras\\utils\\traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m---> 67\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     68\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\PimVeefkind\\miniconda3\\envs\\learn\\lib\\site-packages\\keras\\saving\\save.py:204\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(filepath_str, \u001b[39mstr\u001b[39m):\n\u001b[0;32m    203\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m tf\u001b[39m.\u001b[39mio\u001b[39m.\u001b[39mgfile\u001b[39m.\u001b[39mexists(filepath_str):\n\u001b[1;32m--> 204\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mIOError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mNo file or directory found at \u001b[39m\u001b[39m{\u001b[39;00mfilepath_str\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m    206\u001b[0m   \u001b[39mif\u001b[39;00m tf\u001b[39m.\u001b[39mio\u001b[39m.\u001b[39mgfile\u001b[39m.\u001b[39misdir(filepath_str):\n\u001b[0;32m    207\u001b[0m     \u001b[39mreturn\u001b[39;00m saved_model_load\u001b[39m.\u001b[39mload(filepath_str, \u001b[39mcompile\u001b[39m, options)\n",
      "\u001b[1;31mOSError\u001b[0m: No file or directory found at exp/hyperparams/saved_models/model_sample_True"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error as mae\n",
    "from base.SCINet import scinet_builder\n",
    "\n",
    "\n",
    "def denormalize(x, per_sample,means,stds):\n",
    "\n",
    "    if per_sample:\n",
    "        return(x*stds[0]+means[0])\n",
    "    else:\n",
    "        x_temp = x*stds[1]+means[1]\n",
    "        return(np.exp(x_temp)-1) #luckily there are no negative values in the dataset\n",
    "\n",
    "maes = []\n",
    "\n",
    "for normalize_per_sample in sampling:\n",
    "\n",
    "    model = scinet_builder(\n",
    "                    output_len=  [Y_len],\n",
    "                    input_len = X_len,\n",
    "                    output_dim = [data_proc[\"X_train\"].shape[2]],\n",
    "                    input_dim = data_proc[\"X_train\"].shape[2],\n",
    "                    selected_columns = [[0]], \n",
    "                    loss_weights = [1],\n",
    "                    hid_size = 8,\n",
    "                    num_levels = 4,\n",
    "                    kernel = 5,\n",
    "                    dropout = .5,\n",
    "                    learning_rate = 0.001,)\n",
    "\n",
    "    model.load_weights('exp/hyperparams/saved_models/model_model_sample_{}.h5'.format(normalize_per_sample))\n",
    "    prediction = model.predict(data_proc['X_test'])\n",
    "    print(np.shape(prediction))\n",
    "    for i in tqdm(range(data_test[str(normalize_per_sample)]['X_test'].shape[2])):\n",
    "        maes.append(mae(data_proc[str(normalize_per_sample)]['y_test_unnormalized'][:,:i],prediction[:,:,i]))\n",
    "    \n",
    "    for i in range(data_proc['X_test'].shape[2]):\n",
    "        print('Mae per sample column {} = {}, mae per column column {} = {}'.format(i,np.round(maes[i],3),i,np.round(maes[i+data_proc['X_test'].shape[2]],3)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Model hyperparameters\n",
    "\n",
    "Now that we have establhed that normalising per ... is superior, we can go ahead and vary some other model hyperparameters. Particilarly, we will concern ourselves with the learning rate, hidden size and number of levels of the SCI_tree, however with few adjustments it can be used for other hyperparameters such as which columns to keep as well. We will first fix the preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from base.train_scinet import train_scinet\n",
    "from utils.data_loading import load_data\n",
    "from utils.preprocess_data import preprocess\n",
    "\n",
    "data_format=[\"open\",\"high\",\"low\",\"close\",\"Volume BTC\",\"Volume USDT\",\"tradecount\"]\n",
    "\n",
    "X_len = 256\n",
    "Y_len = 24\n",
    "\n",
    "\n",
    "standardization_settings = {'per_sample': normalize_per_sample,\n",
    "                            'leaky': False,\n",
    "                            'mode': 'log', #only if per sample is false, choose from log, sqrt or lin\n",
    "                            'sqrt_val': 2, #of course only if mode is sqrt\n",
    "                            'total mean': [],\n",
    "                            'total std': []}\n",
    "\n",
    "pairs = [\"HUFL\", \"HULL\", \"MUFL\", \"MULL\", \"LUFL\", \"LULL\", \"OT\"]\n",
    " \n",
    "data, mean ,std = load_data('Binance_BTCUSDT_minute', pairs)\n",
    "\n",
    "data_proc = preprocess(   data = data,\n",
    "                        symbols = pairs,\n",
    "                        data_format = data_format,\n",
    "                        fraction = 1,\n",
    "                        train_frac = .7,\n",
    "                        val_frac = .15,\n",
    "                        test_frac = .15,\n",
    "                        X_LEN = X_len,\n",
    "                        Y_LEN = Y_len,\n",
    "                        OVERLAPPING = True,\n",
    "                        STANDARDIZE = True,\n",
    "                        standardization_settings = standardization_settings\n",
    "                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then design a loop such that the influence on a parameter of choice can be evaluated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = {'hid_size': 16,\n",
    "            'num_levels': 4,\n",
    "            'learning_rate': 0.001}\n",
    "\n",
    "variational_parameter = 'hid_size'\n",
    "parameter_values = [4,8,16,32]\n",
    "\n",
    "for value in parameter_values:\n",
    "\n",
    "    settings[variational_parameter] = value\n",
    "\n",
    "    results = train_scinet( X_train = data_proc[\"X_train\"].astype('float32'),\n",
    "                            y_train = data_proc[\"y_train\"].astype('float32'),\n",
    "                            X_val = data_proc[\"X_val\"].astype('float32'),\n",
    "                            y_val = data_proc[\"y_val\"].astype('float32'),\n",
    "                            X_test = data_proc[\"X_test\"].astype('float32'),\n",
    "                            y_test = data_proc[\"y_test\"].astype('float32'),\n",
    "                            epochs = 1,\n",
    "                            batch_size = 128,\n",
    "                            X_LEN = X_len,\n",
    "                            Y_LEN = [Y_len],\n",
    "                            output_dim = [data_proc[\"X_train\"].shape[2]],\n",
    "                            selected_columns = [[]],\n",
    "                            hid_size= settings['hid_size'],\n",
    "                            num_levels= settings['num_levels'],\n",
    "                            kernel = 5,\n",
    "                            dropout = .5,\n",
    "                            loss_weights= [1],\n",
    "                            learning_rate = settings['learning_rate'],\n",
    "                            probabilistic = False)\n",
    "\n",
    "    results[0].save_weigths('/exp/hyperparams/saved_models/model_{}_{}.h5'.format(variational_parameter,str(value)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will evaluate the performance of the SCINets trained with different hyperparameters. Beware that all models here all loaded so make sure they exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error as mae\n",
    "\n",
    "from base.SCINet import scinet_builder\n",
    "from utils.plotting import plot_barplot\n",
    "\n",
    "settings = {'hid_size': 16,\n",
    "            'num_levels': 4,\n",
    "            'learning_rate': 0.001}\n",
    "\n",
    "\n",
    "maes = []\n",
    "\n",
    "variational_parameter = 'hid_size'\n",
    "parameter_values = [4,8,16,32]\n",
    "\n",
    "for value in parameter_values:\n",
    "\n",
    "    settings[parameter_values] = value\n",
    "\n",
    "    model = scinet_builder(\n",
    "                    output_len=  [Y_len],\n",
    "                    input_len = X_len,\n",
    "                    output_dim = [data_proc[\"X_train\"].shape[2]],\n",
    "                    input_dim = data_proc[\"X_train\"].shape[2],\n",
    "                    selected_columns = [[0]], \n",
    "                    loss_weights = [1],\n",
    "                    hid_size = settings['hid_size'],\n",
    "                    num_levels = settings['num_levels'],\n",
    "                    kernel = 5,\n",
    "                    dropout = .5,\n",
    "                    learning_rate = settings['learning_rate'],)\n",
    "\n",
    "    model.load_weights('exp/hyperparams/saved_models/model_{}_{}.h5'.format(variational_parameter,value))\n",
    "    prediction = model.predict(data_proc['X_test'])\n",
    "    \n",
    "    maes.append(mae(data_proc['y_test'],prediction))\n",
    "    \n",
    "plot_barplot(variational_parameter, parameter_values ,maes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How good is our best model?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('learn')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "b80aa8293e98206ec1521bb25d120a454bd9470ae610e9b13876565475d9d2ee"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
