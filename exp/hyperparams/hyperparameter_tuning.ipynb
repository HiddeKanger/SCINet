{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook explores the effect of various hyperparameters on the performance of SCINet when training on cryptocurrency data, particularly Bitcoin. We will here only address those hyperparameters that are directly accessible via the 'train_scinet' or 'preprocess' functions. Of course, we are aware that effects of different hyperparameters may correlate which means the per parameter sweep as performed here is suboptimal. That said, this approach does grant a lot of insight into the way the different hyperparameters influence the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First the necessary modules en scripts are imported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "\n",
    "cwd = os.getcwd()\n",
    "BASE_DIR = os.path.dirname(os.path.dirname(cwd))\n",
    "\n",
    "sys.path.insert(0, BASE_DIR) #add base to path for relative imports\n",
    "os.chdir('../..')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Per sample versus per column normalisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we will investigate is the effect of the method of normalisation on the data. That is, we can either normalize every sample individually or the column as a whole. In the latter case, we apply a log function before normalising to deal with the different orders of magnitude in some features (this is the case although it is not explicitly shown here). Also, an option to skip the training (and use precomputed models instead) is available in the form of 'train'. Set it to to 'true' in case you want to train explicitly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting data preprocessing...\n",
      "   48740.22  48745.96  48727.47  48727.47.1   2.27206  110730.9135  136.0\n",
      "0  48763.11  48763.12  48736.70    48736.73   5.33108  259880.1205  427.0\n",
      "1  48778.58  48778.58  48750.37    48763.12   6.87389  335219.0368  389.0\n",
      "2  48760.37  48778.58  48746.39    48778.58  10.58951  516291.2896  425.0\n",
      "3  48799.99  48800.00  48756.93    48760.37  12.24525  597357.8390  535.0\n",
      "4  48795.99  48800.00  48795.99    48800.00   7.55759  368810.1891  423.0 (49997, 7)\n",
      "Making train/validation/test splits...\n",
      "Making samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34717/34717 [00:25<00:00, 1387.09it/s]\n",
      "c:\\Users\\PimVeefkind\\Documents\\ADL-Scinet\\SCINet\\exp\\hyperparams\\utils\\preprocess_data.py:143: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  samples = np.array(samples)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7220/7220 [00:05<00:00, 1219.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7220/7220 [00:03<00:00, 1865.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making X-y splits...\n",
      "Starting data preprocessing...\n",
      "   48740.22  48745.96  48727.47  48727.47.1   2.27206  110730.9135  136.0\n",
      "0  48763.11  48763.12  48736.70    48736.73   5.33108  259880.1205  427.0\n",
      "1  48778.58  48778.58  48750.37    48763.12   6.87389  335219.0368  389.0\n",
      "2  48760.37  48778.58  48746.39    48778.58  10.58951  516291.2896  425.0\n",
      "3  48799.99  48800.00  48756.93    48760.37  12.24525  597357.8390  535.0\n",
      "4  48795.99  48800.00  48795.99    48800.00   7.55759  368810.1891  423.0 (49997, 7)\n",
      "Making train/validation/test splits...\n",
      "Making samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34717/34717 [00:20<00:00, 1673.17it/s]\n",
      "c:\\Users\\PimVeefkind\\Documents\\ADL-Scinet\\SCINet\\exp\\hyperparams\\utils\\preprocess_data.py:143: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  samples = np.array(samples)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7220/7220 [00:03<00:00, 2033.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7220/7220 [00:03<00:00, 1988.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making X-y splits...\n"
     ]
    }
   ],
   "source": [
    "from base.train_scinet import train_scinet\n",
    "from utils.data_loading import load_data\n",
    "from utils.preprocess_data import preprocess\n",
    "\n",
    "data_format=[\"open\",\"high\",\"low\",\"close\",\"Volume BTC\",\"Volume USDT\",\"tradecount\"]\n",
    "\n",
    "X_len = 256\n",
    "Y_len = 24\n",
    "\n",
    "sampling = [True,False]\n",
    "\n",
    "means = []\n",
    "stds = []\n",
    "\n",
    "data_test = {}\n",
    "\n",
    "for normalize_per_sample in sampling:\n",
    "\n",
    "    standardization_settings = {'per_sample': normalize_per_sample,\n",
    "                            'leaky': False,\n",
    "                            'mode': 'log', #only if per sample is false, choose from log, sqrt or lin\n",
    "                            'sqrt_val': 2, #of course only if mode is sqrt\n",
    "                            'total mean': [],\n",
    "                            'total std': []}\n",
    "\n",
    "    pairs = [\"HUFL\", \"HULL\", \"MUFL\", \"MULL\", \"LUFL\", \"LULL\", \"OT\"]\n",
    " \n",
    "    data, mean ,std = load_data('Binance_BTCUSDT_minute', pairs)\n",
    "\n",
    "    \n",
    "\n",
    "    data_proc = preprocess(   data = data,\n",
    "                        symbols = pairs,\n",
    "                        data_format = data_format,\n",
    "                        fraction = 1,\n",
    "                        train_frac = .7,\n",
    "                        val_frac = .15,\n",
    "                        test_frac = .15,\n",
    "                        X_LEN = X_len,\n",
    "                        Y_LEN = Y_len,\n",
    "                        OVERLAPPING = True,\n",
    "                        STANDARDIZE = True,\n",
    "                        standardization_settings = standardization_settings\n",
    "                        )\n",
    "\n",
    "    means.append(data_proc['mean'])\n",
    "    stds.append(data_proc['std'])\n",
    "\n",
    "    data_test[str(normalize_per_sample)]= {'X_test': data_proc['X_test'],'y_test_unnormalized': data_proc['y_test_unnormalized']}\n",
    "\n",
    "    train = False\n",
    "    if train:\n",
    "        results = train_scinet( X_train = data_proc[\"X_train\"].astype('float32'),\n",
    "                            y_train = data_proc[\"y_train\"].astype('float32'),\n",
    "                            X_val = data_proc[\"X_val\"].astype('float32'),\n",
    "                            y_val = data_proc[\"y_val\"].astype('float32'),\n",
    "                            X_test = data_proc[\"X_test\"].astype('float32'),\n",
    "                            y_test = data_proc[\"y_test\"].astype('float32'),\n",
    "                            epochs = 1,\n",
    "                            batch_size = 128,\n",
    "                            X_LEN = X_len,\n",
    "                            Y_LEN = [Y_len],\n",
    "                            output_dim = [data_proc[\"X_train\"].shape[2]],\n",
    "                            selected_columns = [[]],\n",
    "                            hid_size= 8,\n",
    "                            num_levels= 4,\n",
    "                            kernel = 5,\n",
    "                            dropout = .5,\n",
    "                            loss_weights= [1],\n",
    "                            learning_rate = 0.001,\n",
    "                            probabilistic = False)\n",
    "\n",
    "        results[0].save_weigths(f'/exp/hyperparams/saved_models/model_sample_{str(normalize_per_sample)}.h5')\n",
    "\n",
    "        \n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to properly compare the performances, we should denormalize the predictions. Otherwise, the best performer will be the one where the normalisations yielded the smallest values. Here, we will only predict the first column as it shortens training considerably. In addition, since we have to denormalize we cannot fairly calculate a mae over all columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "No file or directory found at exp/hyperparams/saved_models/model_sample_True",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\PimVeefkind\\Documents\\ADL-Scinet\\SCINet\\exp\\hyperparams\\hyperparameter_tuning.ipynb Cell 9'\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/PimVeefkind/Documents/ADL-Scinet/SCINet/exp/hyperparams/hyperparameter_tuning.ipynb#ch0000024?line=13'>14</a>\u001b[0m maes \u001b[39m=\u001b[39m []\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/PimVeefkind/Documents/ADL-Scinet/SCINet/exp/hyperparams/hyperparameter_tuning.ipynb#ch0000024?line=15'>16</a>\u001b[0m \u001b[39mfor\u001b[39;00m normalize_per_sample \u001b[39min\u001b[39;00m sampling:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/PimVeefkind/Documents/ADL-Scinet/SCINet/exp/hyperparams/hyperparameter_tuning.ipynb#ch0000024?line=17'>18</a>\u001b[0m     model \u001b[39m=\u001b[39m keras\u001b[39m.\u001b[39;49mmodels\u001b[39m.\u001b[39;49mload_model(\u001b[39m'\u001b[39;49m\u001b[39mexp/hyperparams/saved_models/model_sample_\u001b[39;49m\u001b[39m{}\u001b[39;49;00m\u001b[39m'\u001b[39;49m\u001b[39m.\u001b[39;49mformat(\u001b[39mstr\u001b[39;49m(normalize_per_sample)))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/PimVeefkind/Documents/ADL-Scinet/SCINet/exp/hyperparams/hyperparameter_tuning.ipynb#ch0000024?line=18'>19</a>\u001b[0m     prediction \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(data_proc[\u001b[39m'\u001b[39m\u001b[39mX_test\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/PimVeefkind/Documents/ADL-Scinet/SCINet/exp/hyperparams/hyperparameter_tuning.ipynb#ch0000024?line=19'>20</a>\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(data_test[\u001b[39mstr\u001b[39m(normalize_per_sample)][\u001b[39m'\u001b[39m\u001b[39mX_test\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mshape[\u001b[39m2\u001b[39m])):\n",
      "File \u001b[1;32mc:\\Users\\PimVeefkind\\miniconda3\\envs\\learn\\lib\\site-packages\\keras\\utils\\traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m---> 67\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     68\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\PimVeefkind\\miniconda3\\envs\\learn\\lib\\site-packages\\keras\\saving\\save.py:204\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(filepath_str, \u001b[39mstr\u001b[39m):\n\u001b[0;32m    203\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m tf\u001b[39m.\u001b[39mio\u001b[39m.\u001b[39mgfile\u001b[39m.\u001b[39mexists(filepath_str):\n\u001b[1;32m--> 204\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mIOError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mNo file or directory found at \u001b[39m\u001b[39m{\u001b[39;00mfilepath_str\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m    206\u001b[0m   \u001b[39mif\u001b[39;00m tf\u001b[39m.\u001b[39mio\u001b[39m.\u001b[39mgfile\u001b[39m.\u001b[39misdir(filepath_str):\n\u001b[0;32m    207\u001b[0m     \u001b[39mreturn\u001b[39;00m saved_model_load\u001b[39m.\u001b[39mload(filepath_str, \u001b[39mcompile\u001b[39m, options)\n",
      "\u001b[1;31mOSError\u001b[0m: No file or directory found at exp/hyperparams/saved_models/model_sample_True"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error as mae\n",
    "from base.SCINet import scinet_builder\n",
    "\n",
    "\n",
    "def denormalize(x, per_sample,means,stds):\n",
    "\n",
    "    if per_sample:\n",
    "        return(x*stds[0]+means[0])\n",
    "    else:\n",
    "        x_temp = x*stds[1]+means[1]\n",
    "        return(np.exp(x_temp)-1) #luckily there are no negative values in the dataset\n",
    "\n",
    "maes = []\n",
    "\n",
    "for normalize_per_sample in sampling:\n",
    "\n",
    "    model = scinet_builder(\n",
    "                    output_len=  [Y_len],\n",
    "                    input_len = X_len,\n",
    "                    output_dim = [data_proc[\"X_train\"].shape[2]],\n",
    "                    input_dim = data_proc[\"X_train\"].shape[2],\n",
    "                    selected_columns = [[0]], \n",
    "                    loss_weights = [1],\n",
    "                    hid_size = 8,\n",
    "                    num_levels = 4,\n",
    "                    kernel = 5,\n",
    "                    dropout = .5,\n",
    "                    learning_rate = 0.001,)\n",
    "\n",
    "    model.load_weights('exp/hyperparams/saved_models/model_model_sample_{}.h5'.format(normalize_per_sample))\n",
    "    prediction = model.predict(data_proc['X_test'])\n",
    "    print(np.shape(prediction))\n",
    "    for i in tqdm(range(data_test[str(normalize_per_sample)]['X_test'].shape[2])):\n",
    "        maes.append(mae(data_proc[str(normalize_per_sample)]['y_test_unnormalized'][:,:i],prediction[:,:,i]))\n",
    "    \n",
    "    for i in range(data_proc['X_test'].shape[2]):\n",
    "        print('Mae per sample column {} = {}, mae per column column {} = {}'.format(i,np.round(maes[i],3),i,np.round(maes[i+data_proc['X_test'].shape[2]],3)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Model hyperparameters\n",
    "\n",
    "Now that we have establhed that normalising per ... is superior, we can go ahead and vary some other model hyperparameters. Particilarly, we will concern ourselves with the learning rate, hidden size and number of levels of the SCI_tree, however with few adjustments it can be used for other hyperparameters such as which columns to keep as well. We will first fix the preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from base.train_scinet import train_scinet\n",
    "from utils.data_loading import load_data\n",
    "from utils.preprocess_data import preprocess\n",
    "\n",
    "data_format=[\"open\",\"high\",\"low\",\"close\",\"Volume BTC\",\"Volume USDT\",\"tradecount\"]\n",
    "\n",
    "X_len = 256\n",
    "Y_len = 24\n",
    "\n",
    "\n",
    "standardization_settings = {'per_sample': normalize_per_sample,\n",
    "                            'leaky': False,\n",
    "                            'mode': 'log', #only if per sample is false, choose from log, sqrt or lin\n",
    "                            'sqrt_val': 2, #of course only if mode is sqrt\n",
    "                            'total mean': [],\n",
    "                            'total std': []}\n",
    "\n",
    "pairs = [\"HUFL\", \"HULL\", \"MUFL\", \"MULL\", \"LUFL\", \"LULL\", \"OT\"]\n",
    " \n",
    "data, mean ,std = load_data('Binance_BTCUSDT_minute', pairs)\n",
    "\n",
    "data_proc = preprocess(   data = data,\n",
    "                        symbols = pairs,\n",
    "                        data_format = data_format,\n",
    "                        fraction = 1,\n",
    "                        train_frac = .7,\n",
    "                        val_frac = .15,\n",
    "                        test_frac = .15,\n",
    "                        X_LEN = X_len,\n",
    "                        Y_LEN = Y_len,\n",
    "                        OVERLAPPING = True,\n",
    "                        STANDARDIZE = True,\n",
    "                        standardization_settings = standardization_settings\n",
    "                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then design a loop such that the influence on a parameter of choice can be evaluated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = {'hid_size': 16,\n",
    "            'num_levels': 4,\n",
    "            'learning_rate': 0.001}\n",
    "\n",
    "variational_parameter = 'hid_size'\n",
    "parameter_values = [4,8,16,32]\n",
    "\n",
    "for value in parameter_values:\n",
    "\n",
    "    settings[variational_parameter] = value\n",
    "\n",
    "    results = train_scinet( X_train = data_proc[\"X_train\"].astype('float32'),\n",
    "                            y_train = data_proc[\"y_train\"].astype('float32'),\n",
    "                            X_val = data_proc[\"X_val\"].astype('float32'),\n",
    "                            y_val = data_proc[\"y_val\"].astype('float32'),\n",
    "                            X_test = data_proc[\"X_test\"].astype('float32'),\n",
    "                            y_test = data_proc[\"y_test\"].astype('float32'),\n",
    "                            epochs = 1,\n",
    "                            batch_size = 128,\n",
    "                            X_LEN = X_len,\n",
    "                            Y_LEN = [Y_len],\n",
    "                            output_dim = [data_proc[\"X_train\"].shape[2]],\n",
    "                            selected_columns = [[]],\n",
    "                            hid_size= settings['hid_size'],\n",
    "                            num_levels= settings['num_levels'],\n",
    "                            kernel = 5,\n",
    "                            dropout = .5,\n",
    "                            loss_weights= [1],\n",
    "                            learning_rate = settings['learning_rate'],\n",
    "                            probabilistic = False)\n",
    "\n",
    "    results[0].save_weigths('/exp/hyperparams/saved_models/model_{}_{}.h5'.format(variational_parameter,str(value)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will evaluate the performance of the SCINets trained with different hyperparameters. Beware that all models here all loaded so make sure they exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error as mae\n",
    "from base.SCINet import scinet_builder\n",
    "\n",
    "settings = {'hid_size': 16,\n",
    "            'num_levels': 4,\n",
    "            'learning_rate': 0.001}\n",
    "\n",
    "\n",
    "maes = []\n",
    "\n",
    "variational_parameter = 'hid_size'\n",
    "parameter_values = [4,8,16,32]\n",
    "\n",
    "for value in parameter_values:\n",
    "\n",
    "    settings[parameter_values] = value\n",
    "\n",
    "    model = scinet_builder(\n",
    "                    output_len=  [Y_len],\n",
    "                    input_len = X_len,\n",
    "                    output_dim = [data_proc[\"X_train\"].shape[2]],\n",
    "                    input_dim = data_proc[\"X_train\"].shape[2],\n",
    "                    selected_columns = [[0]], \n",
    "                    loss_weights = [1],\n",
    "                    hid_size = settings['hid_size'],\n",
    "                    num_levels = settings['num_levels'],\n",
    "                    kernel = 5,\n",
    "                    dropout = .5,\n",
    "                    learning_rate = settings['learning_rate'],)\n",
    "\n",
    "    model.load_weights('exp/hyperparams/saved_models/model_{}_{}.h5'.format(variational_parameter,value))\n",
    "    prediction = model.predict(data_proc['X_test'])\n",
    "    \n",
    "    maes.append(mae(data_proc['y_test'],prediction))\n",
    "    \n",
    "plot_barplot(variational_parameter, parameter_values ,maes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the model can be trained. At first the hyperparameters which are not being optimized are defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "BATCH_SIZE = 8\n",
    "HID_SIZE = 4\n",
    "NUM_LEVELS = 3\n",
    "KERNEL_SIZE = 5\n",
    "DROPOUT = 0.5\n",
    "PROBABILISTIC = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then some values of the parameter to be tuned (in this case the learning rate) are defined. For each value of this parameter a model is trained and it's performance on the validation set is saved for plotting later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================[SCINET]=====================================\n",
      "Initializing training with data:\n",
      "X_train: (34925, 48, 7), y_train: (34925, 24, 7)\n",
      "X_val: (7428, 48, 7), y_val: (7428, 24, 7)\n",
      "X_test: (7428, 48, 7), y_test: (7428, 24, 7)\n",
      "Building model...\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 48, 7)]           0         \n",
      "                                                                 \n",
      " Block_0 (SCINet)            (None, 24, 7)             97332     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 97,332\n",
      "Trainable params: 97,332\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Is null X: 0\n",
      "Is null y: 0\n",
      "Epoch 1/10\n",
      "4366/4366 [==============================] - 185s 38ms/step - loss: 0.3745 - val_loss: 0.3429\n",
      "Epoch 2/10\n",
      "4366/4366 [==============================] - 186s 43ms/step - loss: 0.3080 - val_loss: 0.2840\n",
      "Epoch 3/10\n",
      "3767/4366 [========================>.....] - ETA: 24s - loss: 0.3022"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\PimVeefkind\\Documents\\ADL-Scinet\\SCINet\\exp\\hyperparams\\hyperparameter_tuning.ipynb Cell 15'\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/PimVeefkind/Documents/ADL-Scinet/SCINet/exp/hyperparams/hyperparameter_tuning.ipynb#ch0000014?line=3'>4</a>\u001b[0m val_losses \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros((\u001b[39mlen\u001b[39m(LEARNING_RATES), EPOCHS))\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/PimVeefkind/Documents/ADL-Scinet/SCINet/exp/hyperparams/hyperparameter_tuning.ipynb#ch0000014?line=4'>5</a>\u001b[0m \u001b[39mfor\u001b[39;00m idx, LEARNING_RATE \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(LEARNING_RATES):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/PimVeefkind/Documents/ADL-Scinet/SCINet/exp/hyperparams/hyperparameter_tuning.ipynb#ch0000014?line=6'>7</a>\u001b[0m     model, history, X_train , y_train, X_val, y_val, X_test, y_test \u001b[39m=\u001b[39m train_scinet( X_train \u001b[39m=\u001b[39;49m data_proc[\u001b[39m\"\u001b[39;49m\u001b[39mX_train\u001b[39;49m\u001b[39m\"\u001b[39;49m]\u001b[39m.\u001b[39;49mastype(\u001b[39m'\u001b[39;49m\u001b[39mfloat32\u001b[39;49m\u001b[39m'\u001b[39;49m),\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/PimVeefkind/Documents/ADL-Scinet/SCINet/exp/hyperparams/hyperparameter_tuning.ipynb#ch0000014?line=7'>8</a>\u001b[0m                                                                                     y_train \u001b[39m=\u001b[39;49m data_proc[\u001b[39m\"\u001b[39;49m\u001b[39my_train\u001b[39;49m\u001b[39m\"\u001b[39;49m]\u001b[39m.\u001b[39;49mastype(\u001b[39m'\u001b[39;49m\u001b[39mfloat32\u001b[39;49m\u001b[39m'\u001b[39;49m),\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/PimVeefkind/Documents/ADL-Scinet/SCINet/exp/hyperparams/hyperparameter_tuning.ipynb#ch0000014?line=8'>9</a>\u001b[0m                                                                                     X_val \u001b[39m=\u001b[39;49m data_proc[\u001b[39m\"\u001b[39;49m\u001b[39mX_val\u001b[39;49m\u001b[39m\"\u001b[39;49m]\u001b[39m.\u001b[39;49mastype(\u001b[39m'\u001b[39;49m\u001b[39mfloat32\u001b[39;49m\u001b[39m'\u001b[39;49m),\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/PimVeefkind/Documents/ADL-Scinet/SCINet/exp/hyperparams/hyperparameter_tuning.ipynb#ch0000014?line=9'>10</a>\u001b[0m                                                                                     y_val \u001b[39m=\u001b[39;49m data_proc[\u001b[39m\"\u001b[39;49m\u001b[39my_val\u001b[39;49m\u001b[39m\"\u001b[39;49m]\u001b[39m.\u001b[39;49mastype(\u001b[39m'\u001b[39;49m\u001b[39mfloat32\u001b[39;49m\u001b[39m'\u001b[39;49m),\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/PimVeefkind/Documents/ADL-Scinet/SCINet/exp/hyperparams/hyperparameter_tuning.ipynb#ch0000014?line=10'>11</a>\u001b[0m                                                                                     X_test \u001b[39m=\u001b[39;49m data_proc[\u001b[39m\"\u001b[39;49m\u001b[39mX_test\u001b[39;49m\u001b[39m\"\u001b[39;49m]\u001b[39m.\u001b[39;49mastype(\u001b[39m'\u001b[39;49m\u001b[39mfloat32\u001b[39;49m\u001b[39m'\u001b[39;49m),\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/PimVeefkind/Documents/ADL-Scinet/SCINet/exp/hyperparams/hyperparameter_tuning.ipynb#ch0000014?line=11'>12</a>\u001b[0m                                                                                     y_test \u001b[39m=\u001b[39;49m data_proc[\u001b[39m\"\u001b[39;49m\u001b[39my_test\u001b[39;49m\u001b[39m\"\u001b[39;49m]\u001b[39m.\u001b[39;49mastype(\u001b[39m'\u001b[39;49m\u001b[39mfloat32\u001b[39;49m\u001b[39m'\u001b[39;49m),\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/PimVeefkind/Documents/ADL-Scinet/SCINet/exp/hyperparams/hyperparameter_tuning.ipynb#ch0000014?line=12'>13</a>\u001b[0m                                                                                     epochs \u001b[39m=\u001b[39;49m EPOCHS,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/PimVeefkind/Documents/ADL-Scinet/SCINet/exp/hyperparams/hyperparameter_tuning.ipynb#ch0000014?line=13'>14</a>\u001b[0m                                                                                     batch_size \u001b[39m=\u001b[39;49m BATCH_SIZE,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/PimVeefkind/Documents/ADL-Scinet/SCINet/exp/hyperparams/hyperparameter_tuning.ipynb#ch0000014?line=14'>15</a>\u001b[0m                                                                                     X_LEN \u001b[39m=\u001b[39;49m X_len,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/PimVeefkind/Documents/ADL-Scinet/SCINet/exp/hyperparams/hyperparameter_tuning.ipynb#ch0000014?line=15'>16</a>\u001b[0m                                                                                     Y_LEN \u001b[39m=\u001b[39;49m [Y_len],\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/PimVeefkind/Documents/ADL-Scinet/SCINet/exp/hyperparams/hyperparameter_tuning.ipynb#ch0000014?line=16'>17</a>\u001b[0m                                                                                     output_dim \u001b[39m=\u001b[39;49m [data_proc[\u001b[39m\"\u001b[39;49m\u001b[39mX_train\u001b[39;49m\u001b[39m\"\u001b[39;49m]\u001b[39m.\u001b[39;49mshape[\u001b[39m2\u001b[39;49m]],\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/PimVeefkind/Documents/ADL-Scinet/SCINet/exp/hyperparams/hyperparameter_tuning.ipynb#ch0000014?line=17'>18</a>\u001b[0m                                                                                     selected_columns \u001b[39m=\u001b[39;49m \u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/PimVeefkind/Documents/ADL-Scinet/SCINet/exp/hyperparams/hyperparameter_tuning.ipynb#ch0000014?line=18'>19</a>\u001b[0m                                                                                     hid_size\u001b[39m=\u001b[39;49m HID_SIZE,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/PimVeefkind/Documents/ADL-Scinet/SCINet/exp/hyperparams/hyperparameter_tuning.ipynb#ch0000014?line=19'>20</a>\u001b[0m                                                                                     num_levels\u001b[39m=\u001b[39;49m NUM_LEVELS,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/PimVeefkind/Documents/ADL-Scinet/SCINet/exp/hyperparams/hyperparameter_tuning.ipynb#ch0000014?line=20'>21</a>\u001b[0m                                                                                     kernel \u001b[39m=\u001b[39;49m KERNEL_SIZE,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/PimVeefkind/Documents/ADL-Scinet/SCINet/exp/hyperparams/hyperparameter_tuning.ipynb#ch0000014?line=21'>22</a>\u001b[0m                                                                                     dropout \u001b[39m=\u001b[39;49m DROPOUT,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/PimVeefkind/Documents/ADL-Scinet/SCINet/exp/hyperparams/hyperparameter_tuning.ipynb#ch0000014?line=22'>23</a>\u001b[0m                                                                                     loss_weights\u001b[39m=\u001b[39;49m [\u001b[39m1\u001b[39;49m],\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/PimVeefkind/Documents/ADL-Scinet/SCINet/exp/hyperparams/hyperparameter_tuning.ipynb#ch0000014?line=23'>24</a>\u001b[0m                                                                                     learning_rate \u001b[39m=\u001b[39;49m LEARNING_RATE,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/PimVeefkind/Documents/ADL-Scinet/SCINet/exp/hyperparams/hyperparameter_tuning.ipynb#ch0000014?line=24'>25</a>\u001b[0m                                                                                     probabilistic \u001b[39m=\u001b[39;49m PROBABILISTIC)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/PimVeefkind/Documents/ADL-Scinet/SCINet/exp/hyperparams/hyperparameter_tuning.ipynb#ch0000014?line=26'>27</a>\u001b[0m     train_loss \u001b[39m=\u001b[39m history\u001b[39m.\u001b[39mhistory[\u001b[39m'\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/PimVeefkind/Documents/ADL-Scinet/SCINet/exp/hyperparams/hyperparameter_tuning.ipynb#ch0000014?line=27'>28</a>\u001b[0m     train_losses[idx] \u001b[39m=\u001b[39m train_loss\n",
      "File \u001b[1;32mc:\\Users\\PimVeefkind\\Documents\\ADL-Scinet\\SCINet\\base\\train_scinet.py:96\u001b[0m, in \u001b[0;36mtrain_scinet\u001b[1;34m(X_train, y_train, X_val, y_val, X_test, y_test, epochs, batch_size, X_LEN, Y_LEN, output_dim, selected_columns, hid_size, num_levels, kernel, dropout, loss_weights, learning_rate, probabilistic)\u001b[0m\n\u001b[0;32m     86\u001b[0m     history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mfit(\n\u001b[0;32m     87\u001b[0m         X_train, \n\u001b[0;32m     88\u001b[0m         [y_train[:, :, selected_columns[i]] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(selected_columns))], \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     93\u001b[0m         callbacks\u001b[39m=\u001b[39m[callback]\n\u001b[0;32m     94\u001b[0m         )\n\u001b[0;32m     95\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 96\u001b[0m     history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(\n\u001b[0;32m     97\u001b[0m         X_train, \n\u001b[0;32m     98\u001b[0m         [y_train] \u001b[39m*\u001b[39;49m \u001b[39mlen\u001b[39;49m(output_dim), \n\u001b[0;32m     99\u001b[0m         epochs \u001b[39m=\u001b[39;49m epochs, \n\u001b[0;32m    100\u001b[0m         batch_size \u001b[39m=\u001b[39;49m batch_size, \n\u001b[0;32m    101\u001b[0m         validation_data \u001b[39m=\u001b[39;49m (X_val,\n\u001b[0;32m    102\u001b[0m         [y_val] \u001b[39m*\u001b[39;49m \u001b[39mlen\u001b[39;49m(output_dim)),\n\u001b[0;32m    103\u001b[0m         callbacks\u001b[39m=\u001b[39;49m[callback]\n\u001b[0;32m    104\u001b[0m         )\n\u001b[0;32m    107\u001b[0m     \u001b[39m#store performances\u001b[39;00m\n\u001b[0;32m    109\u001b[0m \u001b[39mreturn\u001b[39;00m model, history, X_train , y_train, X_val, y_val, X_test, y_test\n",
      "File \u001b[1;32mc:\\Users\\PimVeefkind\\miniconda3\\envs\\learn\\lib\\site-packages\\keras\\utils\\traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 64\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\PimVeefkind\\miniconda3\\envs\\learn\\lib\\site-packages\\keras\\engine\\training.py:1384\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1377\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1378\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m   1379\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   1380\u001b[0m     step_num\u001b[39m=\u001b[39mstep,\n\u001b[0;32m   1381\u001b[0m     batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[0;32m   1382\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m   1383\u001b[0m   callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1384\u001b[0m   tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   1385\u001b[0m   \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1386\u001b[0m     context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\PimVeefkind\\miniconda3\\envs\\learn\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\PimVeefkind\\miniconda3\\envs\\learn\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\PimVeefkind\\miniconda3\\envs\\learn\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    945\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateless_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    950\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    951\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\PimVeefkind\\miniconda3\\envs\\learn\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2956\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2953\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m   2954\u001b[0m   (graph_function,\n\u001b[0;32m   2955\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2956\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m   2957\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[1;32mc:\\Users\\PimVeefkind\\miniconda3\\envs\\learn\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1853\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1849\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1850\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1851\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1852\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1853\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[0;32m   1854\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[0;32m   1855\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1856\u001b[0m     args,\n\u001b[0;32m   1857\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1858\u001b[0m     executing_eagerly)\n\u001b[0;32m   1859\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\PimVeefkind\\miniconda3\\envs\\learn\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[0;32m    498\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 499\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    500\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[0;32m    501\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[0;32m    502\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m    503\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m    504\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[0;32m    505\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    506\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    507\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[0;32m    508\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    511\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[0;32m    512\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mc:\\Users\\PimVeefkind\\miniconda3\\envs\\learn\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "LEARNING_RATES = [0.001]\n",
    "\n",
    "train_losses = np.zeros((len(LEARNING_RATES), EPOCHS))\n",
    "val_losses = np.zeros((len(LEARNING_RATES), EPOCHS))\n",
    "for idx, LEARNING_RATE in enumerate(LEARNING_RATES):\n",
    "\n",
    "    model, history, X_train , y_train, X_val, y_val, X_test, y_test = train_scinet( X_train = data_proc[\"X_train\"].astype('float32'),\n",
    "                                                                                    y_train = data_proc[\"y_train\"].astype('float32'),\n",
    "                                                                                    X_val = data_proc[\"X_val\"].astype('float32'),\n",
    "                                                                                    y_val = data_proc[\"y_val\"].astype('float32'),\n",
    "                                                                                    X_test = data_proc[\"X_test\"].astype('float32'),\n",
    "                                                                                    y_test = data_proc[\"y_test\"].astype('float32'),\n",
    "                                                                                    epochs = EPOCHS,\n",
    "                                                                                    batch_size = BATCH_SIZE,\n",
    "                                                                                    X_LEN = X_len,\n",
    "                                                                                    Y_LEN = [Y_len],\n",
    "                                                                                    output_dim = [data_proc[\"X_train\"].shape[2]],\n",
    "                                                                                    selected_columns = None,\n",
    "                                                                                    hid_size= HID_SIZE,\n",
    "                                                                                    num_levels= NUM_LEVELS,\n",
    "                                                                                    kernel = KERNEL_SIZE,\n",
    "                                                                                    dropout = DROPOUT,\n",
    "                                                                                    loss_weights= [1],\n",
    "                                                                                    learning_rate = LEARNING_RATE,\n",
    "                                                                                    probabilistic = PROBABILISTIC)\n",
    "\n",
    "    train_loss = history.history['loss']\n",
    "    train_losses[idx] = train_loss\n",
    "\n",
    "    val_loss = history.history['val_loss']\n",
    "    val_losses[idx] = val_loss\n",
    "    \n",
    "    #model.save(f'saved_models/model_learning_rate_{LEARNING_RATE}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the performance of each model on the validation set is compared using a plot. The hyperparamer of the model with the lowest loss in the validation set can be selected as the optimal value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utils.plotting'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/45/tkfph_d93ss1d6wmks7ggsg00000gn/T/ipykernel_65006/87258678.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplotting\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mplot_barplot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mhyperparameter_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'LearningRate'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'utils.plotting'"
     ]
    }
   ],
   "source": [
    "\n",
    "from utils.plotting import plot_barplot\n",
    "\n",
    "\n",
    "hyperparameter_type='LearningRate'\n",
    "\n",
    "\n",
    "\n",
    "plot_barplot(LEARNING_RATES, val_losses, hyperparameter_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('learn')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "b80aa8293e98206ec1521bb25d120a454bd9470ae610e9b13876565475d9d2ee"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
