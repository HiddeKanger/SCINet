{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef8887ba",
   "metadata": {},
   "source": [
    "# Reproducing results\n",
    "\n",
    "This notebook shows how the results produced by the original paper can be reproduced by running several experiments on different datasets. For the reproduction, the original datasets from the paper are used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44519aae",
   "metadata": {},
   "source": [
    "First, we want to do some imports. We also want to define our working directory path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d30bd799",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.font_manager import json_dump\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "\n",
    "#cwd = os.getcwd()\n",
    "\n",
    "#WORKDIR_PATH = os.path.dirname(os.path.realpath(cwd)) + \"/../../\"\n",
    "#sys.path.insert(1, WORKDIR_PATH)\n",
    "\n",
    "\n",
    "cwd = os.getcwd()\n",
    "WORKDIR_PATH = os.path.dirname(os.path.dirname(cwd+'/../../'))\n",
    "\n",
    "sys.path.insert(0, WORKDIR_PATH)\n",
    "\n",
    "from preprocess_data import preprocess\n",
    "from base.train_scinet import train_scinet\n",
    "\n",
    "WORKDIR_PATH = os.path.dirname(os.path.dirname(cwd+'/reprod/'))\n",
    "sys.path.insert(1, WORKDIR_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b967243",
   "metadata": {},
   "source": [
    "Next, we need to load the data, in this case we will use the \"ETTh1.csv' dataset, and do some preprocessing steps before we can use it for training. We indicate what data-format is used, we define the training, validation and test fractions, and the horizon and lookback-window we want to use for training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "90645525",
   "metadata": {},
   "outputs": [],
   "source": [
    "#============= Preprocessing ==============\n",
    "\n",
    "#data_format = [\"timestamp\",\"open\",\"high\",\"low\",\"close\",\"volume\",]\n",
    "data_format = [\"price\"]\n",
    "                    \n",
    "fraction_used = 1\n",
    "train_frac = 0.6\n",
    "val_frac = 0.2\n",
    "test_frac = 0.2\n",
    "\n",
    "X_LEN = 48\n",
    "Y_LEN = 24\n",
    "RANDOM_SEED = 4321#None\n",
    "OVERLAPPING = True\n",
    "STANDARDIZE = True\n",
    "\n",
    "standardization_settings = {'per_sample': True,\n",
    "                            'leaky': False,\n",
    "                            'mode': 'log', #only if per sample is false, choose from log, sqrt or lin\n",
    "                            'sqrt_val': 2, #of course only if mode is sqrt\n",
    "                            'total mean': [],\n",
    "                            'total std': []}\n",
    "\n",
    "pairs = [\"HUFL\", \"HULL\", \"MUFL\", \"MULL\", \"LUFL\", \"LULL\", \"OT\"]\n",
    "\n",
    "#df = pd.read_csv(os.path.dirname(os.path.dirname(cwd+'/data/Data_preprocessed/ETTh1.csv'))).dropna()\n",
    "df = pd.read_csv(f\"/Users/lindsayspoor/Library/Mobile Documents/com~apple~CloudDocs/Documents/Studiedocumenten/2021-2022/ADL/SCINet_repo/exp/reprod/data/Data_preprocessed/ETTh1.csv\").dropna()\n",
    "df = df.swapaxes(\"index\", \"columns\")\n",
    "\n",
    "data = {}\n",
    "for idx, pair in enumerate(pairs):\n",
    "    data[pair] = df.iloc[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a306315",
   "metadata": {},
   "source": [
    "Next, we want to start the training process, i.e. training SCINet on the dataset. For more instructions on how the training works, follow the instructions in the training_scinet.ipynb notebook. The number of training epochs is set to 150 and the hyperparameters are corresponding with the original set of hyperparameters from the original code:\n",
    "\n",
    "https://github.com/cure-lab/SCINet/blob/main/Appendix/Appendix.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3f5a51ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting data preprocessing...\n",
      "Making train/validation/test splits...\n",
      "Making samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 10379/10379 [00:00<00:00, 12276.64it/s]\n",
      "/Users/lindsayspoor/Library/Mobile Documents/com~apple~CloudDocs/Documents/Studiedocumenten/2021-2022/ADL/SCINet_repo/exp/reprod/preprocess_data.py:128: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  samples = np.array(samples)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 3411/3411 [00:00<00:00, 10614.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 3411/3411 [00:00<00:00, 10948.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making X-y splits...\n",
      "===========================[SCINET]=====================================\n",
      "Initializing training with data:\n",
      "X_train: (10340, 48, 7), y_train: (10340, 24, 7)\n",
      "X_val: (3411, 48, 7), y_val: (3411, 24, 7)\n",
      "X_test: (3411, 48, 7), y_test: (3411, 24, 7)\n",
      "Building model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-17 20:05:29.385927: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 48, 7)]           0         \n",
      "_________________________________________________________________\n",
      "Block_0 (SCINet)             (None, 24, 7)             97332     \n",
      "=================================================================\n",
      "Total params: 97,332\n",
      "Trainable params: 97,332\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Is null X: 0\n",
      "Is null y: 0\n",
      "Epoch 1/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-17 20:05:30.799811: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 364/1293 [=======>......................] - ETA: 12s - loss: 1.0100"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/45/tkfph_d93ss1d6wmks7ggsg00000gn/T/ipykernel_20896/849612557.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m                         )\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m model, history, X_train , y_train, X_val, y_val, X_test, y_test = train_scinet( X_train = results[\"X_train\"].astype('float32'),\n\u001b[0m\u001b[1;32m     28\u001b[0m                                                                                 \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"y_train\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'float32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m                                                                                 \u001b[0mX_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"X_val\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'float32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Mobile Documents/com~apple~CloudDocs/Documents/Studiedocumenten/2021-2022/ADL/SCINet_repo/base/train_scinet.py\u001b[0m in \u001b[0;36mtrain_scinet\u001b[0;34m(X_train, y_train, X_val, y_val, X_test, y_test, epochs, batch_size, X_LEN, Y_LEN, output_dim, selected_columns, hid_size, num_levels, kernel, dropout, loss_weights, learning_rate, probabilistic)\u001b[0m\n\u001b[1;32m     94\u001b[0m             )\n\u001b[1;32m     95\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         history = model.fit(\n\u001b[0m\u001b[1;32m     97\u001b[0m             \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0;34m[\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tensorflow/lib/python3.8/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1182\u001b[0m                 _r=1):\n\u001b[1;32m   1183\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1184\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1185\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tensorflow/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tensorflow/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    915\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tensorflow/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3037\u001b[0m       (graph_function,\n\u001b[1;32m   3038\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 3039\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   3040\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   3041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tensorflow/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1961\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1962\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1963\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1964\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1965\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/.conda/envs/tensorflow/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    589\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 591\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    592\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tensorflow/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#================ Training ====================\n",
    "\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 8\n",
    "HID_SIZE = 4\n",
    "NUM_LEVELS = 3\n",
    "KERNEL_SIZE = 5\n",
    "DROPOUT = 0.5\n",
    "LEARNING_RATE = 0.003\n",
    "PROBABILISTIC = False\n",
    "\n",
    "# Process data:\n",
    "results = preprocess(   data = data, \n",
    "                        symbols = pairs,\n",
    "                        data_format = data_format,\n",
    "                        fraction = fraction_used,\n",
    "                        train_frac = train_frac,\n",
    "                        val_frac = val_frac,\n",
    "                        test_frac = test_frac,\n",
    "                        X_LEN = X_LEN,\n",
    "                        Y_LEN = Y_LEN,\n",
    "                        OVERLAPPING = OVERLAPPING,\n",
    "                        STANDARDIZE = STANDARDIZE,\n",
    "                        standardization_settings = standardization_settings\n",
    "                        )\n",
    "\n",
    "model, history, X_train , y_train, X_val, y_val, X_test, y_test = train_scinet( X_train = results[\"X_train\"].astype('float32'),\n",
    "                                                                                y_train = results[\"y_train\"].astype('float32'),\n",
    "                                                                                X_val = results[\"X_val\"].astype('float32'),\n",
    "                                                                                y_val = results[\"y_val\"].astype('float32'),\n",
    "                                                                                X_test = results[\"X_test\"].astype('float32'),\n",
    "                                                                                y_test = results[\"y_test\"].astype('float32'),\n",
    "                                                                                epochs = EPOCHS,\n",
    "                                                                                batch_size = BATCH_SIZE,\n",
    "                                                                                X_LEN = X_LEN,\n",
    "                                                                                Y_LEN = [Y_LEN],\n",
    "                                                                                output_dim = [results[\"X_train\"].shape[2]],\n",
    "                                                                                selected_columns = None,\n",
    "                                                                                hid_size= HID_SIZE,\n",
    "                                                                                num_levels= NUM_LEVELS,\n",
    "                                                                                kernel = KERNEL_SIZE,\n",
    "                                                                                dropout = DROPOUT,\n",
    "                                                                                loss_weights= [1],\n",
    "                                                                                learning_rate = LEARNING_RATE,\n",
    "                                                                                probabilistic = PROBABILISTIC)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f2192b",
   "metadata": {},
   "source": [
    "# Results \n",
    "\n",
    "In order to see whether our training and validation loss meet the results from the original paper, we need to set a target. The targets for MAEs of the corresponding datasets can be found in table 2 in the original paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fe6c9402",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/45/tkfph_d93ss1d6wmks7ggsg00000gn/T/ipykernel_20896/3246852640.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.379\u001b[0m \u001b[0;31m#value of MAE of paper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "target = 0.379 #value of MAE of paper\n",
    "\n",
    "X = np.arange(len(train_loss))\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(X, train_loss, label='Training set')\n",
    "plt.plot(X, val_loss, label=\"Validation set\")\n",
    "plt.axhline(y=target, color='r', linestyle='--', label=\"Paper's result\")\n",
    "plt.xlabel('Epochs', fontsize=15)\n",
    "plt.ylabel('Mean absolute error', fontsize=15)\n",
    "plt.xlim(xmin=0)\n",
    "plt.ylim(ymin=0)\n",
    "plt.title('ETTh1', fontsize=15)\n",
    "plt.legend()\n",
    "#plt.savefig(f\"/Users/lindsayspoor/Library/Mobile Documents/com~apple~CloudDocs/Documents/Studiedocumenten/2021-2022/ADL/SCINet_repo/exp/reprod/results/loss_ETTh1_{Y_LEN}.pdf\")\n",
    "plt.show()\n",
    "\n",
    "output = model(X_test)\n",
    "\n",
    "series = 0\n",
    "total_timesteps = 1000\n",
    "\n",
    "X_time_blocks = math.floor( total_timesteps / X_LEN )\n",
    "Y_time_blocks = math.floor( total_timesteps / Y_LEN )\n",
    "\n",
    "actual_prices = np.array([])\n",
    "for t in range(X_time_blocks):\n",
    "    actual_prices = np.append(actual_prices, X_test[t*X_LEN,:,series])\n",
    "\n",
    "predicted_prices = np.array([])\n",
    "for t in range(Y_time_blocks):\n",
    "    predicted_prices = np.append(predicted_prices, np.array(output[t*Y_LEN])[:,series])\n",
    "\n",
    "X_times = np.arange(len(actual_prices))\n",
    "Y_times = np.arange(len(predicted_prices))\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(X_times, actual_prices, label='actual prices')\n",
    "plt.plot(Y_times, predicted_prices, label='predicted prices')\n",
    "plt.xlabel('Time', fontsize=15)\n",
    "plt.ylabel('Output', fontsize=15)\n",
    "plt.title('ETTh1', fontsize=15)\n",
    "plt.legend()\n",
    "#plt.savefig(f\"/Users/lindsayspoor/Library/Mobile Documents/com~apple~CloudDocs/Documents/Studiedocumenten/2021-2022/ADL/SCINet_repo/exp/reprod/results/predictions_ETTh1_{Y_LEN}.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb40c35e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
