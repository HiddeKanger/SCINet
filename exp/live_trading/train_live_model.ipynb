{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a SCINet Model for Live Trading Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\Studie\\Universiteit\\Studie\\Jaar 4\\Advances in Deep Learning\\SCINet\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "cwd = os.getcwd()\n",
    "BASE_DIR = os.path.dirname(os.path.dirname(cwd))\n",
    "print(BASE_DIR)\n",
    "sys.path.insert(0, BASE_DIR) #add base to path for relative imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from base.train_scinet import train_scinet\n",
    "from base.preprocess_data import preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from time import time\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data from the `data/` folder using an expected OHLCV format as common in stock market data structures. No volume is used in the live data as sparse input data results in 0 volume values a lot of the time as, for demonstration purposes, the timescale is seconds instead of minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#expected dataformat of individual pairs\n",
    "data_format = [\n",
    "                            \"timestamp\",\n",
    "                            \"open\",\n",
    "                            \"high\",\n",
    "                            \"low\",\n",
    "                            \"close\",\n",
    "            ]\n",
    "\n",
    "#fraction of dataset used (could be 1, not that the first samples in the dataset are used)\n",
    "fraction_used = 0.01\n",
    "\n",
    "#train validation test set fractions of used data\n",
    "train_frac = 0.7\n",
    "val_frac = 0.2\n",
    "test_frac = 0.1\n",
    "\n",
    "#predict next Y values based on previous X values\n",
    "X_LEN = 240\n",
    "Y_LEN = 24\n",
    "\n",
    "OVERLAPPING = True\n",
    "STANDARDIZE = True\n",
    "\n",
    "RANDOM_SEED = None\n",
    "\n",
    "if RANDOM_SEED != None:\n",
    "    random.seed(RANDOM_SEED)\n",
    "\n",
    "#names of pairs\n",
    "pairs = [\"BTCUSD\"]\n",
    "data = {}\n",
    "\n",
    "for pair in pairs:\n",
    "    data[pair] =  pd.read_csv(f\"data/{pair}.csv\")#.iloc[:10000, :] #debug\n",
    "    # print(data[pair].isnull().values.any())\n",
    "    \n",
    "data[\"BTCUSD\"] = data[\"BTCUSD\"].loc[ : , data[\"BTCUSD\"].columns != \"volume\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before the network is trained the input data is preprocessed as to standardize the input samples and make train-validation-test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting data preprocessing...\n",
      "     open    high     low   close\n",
      "0   93.25   93.30   93.25   93.30\n",
      "1  100.00  100.00  100.00  100.00\n",
      "2   93.30   93.30   93.30   93.30\n",
      "3   93.35   93.47   93.35   93.47\n",
      "4   93.47   93.47   93.47   93.47 (34653, 4)\n",
      "Making train/validation/test splits...\n",
      "Making samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23993/23993 [00:02<00:00, 9315.17it/s] \n",
      "d:\\Studie\\Universiteit\\Studie\\Jaar 4\\Advances in Deep Learning\\SCINet\\base\\preprocess_data.py:138: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  samples = np.array(samples)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6666/6666 [00:00<00:00, 8679.81it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3202/3202 [00:00<00:00, 6994.04it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making X-y splits...\n",
      "X_train: (23993, 240, 4)\n",
      "y_train: (23993, 24, 4)\n",
      "X_val: (6666, 240, 4)\n",
      "y_val: (6666, 24, 4)\n",
      "X_test: (3202, 240, 4)\n",
      "y_test: (3202, 24, 4)\n"
     ]
    }
   ],
   "source": [
    "results = preprocess(   data = data, \n",
    "                        symbols = pairs,\n",
    "                        data_format = data_format,\n",
    "                        fraction = fraction_used,\n",
    "                        train_frac = train_frac,\n",
    "                        val_frac = val_frac,\n",
    "                        test_frac = test_frac,\n",
    "                        X_LEN = X_LEN,\n",
    "                        Y_LEN = Y_LEN,\n",
    "                        OVERLAPPING = OVERLAPPING,\n",
    "                        STANDARDIZE = STANDARDIZE,\n",
    "                        standardization_settings = {\"per_sample\": True}\n",
    "                    )\n",
    "\n",
    "for result in results:\n",
    "    print(f\"{result}: {results[result].shape}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below trains a SCINet model using the preprocessed data above. All the arguments are explained below in comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================[SCINET]=====================================\n",
      "Initializing training with data:\n",
      "X_train: (23993, 240, 4), y_train: (23993, 24, 4)\n",
      "X_val: (6666, 240, 4), y_val: (6666, 24, 4)\n",
      "X_test: (3202, 240, 4), y_test: (3202, 24, 4)\n",
      "Building model...\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 240, 4)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Block_0 (SCINet)                (None, 24, 4)        259440      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf.concat (TFOpLambda)          (None, 264, 4)       0           input_1[0][0]                    \n",
      "                                                                 Block_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "cropping1d (Cropping1D)         (None, 240, 4)       0           tf.concat[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Block_1 (SCINet)                (None, 24, 4)        259440      cropping1d[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 518,880\n",
      "Trainable params: 518,880\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Is null X: 0\n",
      "Is null y: 0\n",
      "69/69 [==============================] - 281s 3s/step - loss: 1.0488 - Block_0_loss: 1.1301 - Block_1_loss: 0.9947 - val_loss: 0.7629 - val_Block_0_loss: 0.8708 - val_Block_1_loss: 0.6910\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 16\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "N_BLOCKS = 2\n",
    "\n",
    "training_result  = train_scinet(   \n",
    "                X_train = results[\"X_train\"], #samples of training set\n",
    "                y_train = results[\"y_train\"], #labels of training set\n",
    "                X_val = results[\"X_val\"], #samples of validation set\n",
    "                y_val = results[\"y_val\"], #labels of validation set\n",
    "                X_test = results[\"X_test\"], #samples of test set\n",
    "                y_test = results[\"y_test\"], #labels of test set\n",
    "                X_LEN = X_LEN, #input sample length\n",
    "                Y_LEN = [Y_LEN] * N_BLOCKS, #model output sequence length (per scinet stack)\n",
    "                epochs = EPOCHS,\n",
    "                batch_size = BATCH_SIZE,\n",
    "                output_dim = [results[\"X_train\"].shape[2]] * N_BLOCKS, #output dimensions (per scinet stack)\n",
    "                selected_columns = None, #select on which column is trained, None means all\n",
    "                hid_size= 32, #hidden size\n",
    "                num_levels = 3, #number of scinet levels\n",
    "                kernel = 5, #kernel size\n",
    "                dropout = 0.5, #dropout probability\n",
    "                loss_weights = [0.4, 0.6], #combine scinet losses with fractions (must sum up to 1)\n",
    "                probabilistic = False #probabilistic output\n",
    "            )\n",
    "\n",
    "model = training_result[0]\n",
    "\n",
    "model.save_weights(f\"model_weights/{'_'.join(pairs)}_{int(time())}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.13 ('IDL')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0e883bdd8e75fb51e0b3ef9bce90386773c51c30bd0ece054f28d8a267d239d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
